{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from FishDataset import FishDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FishDataset('../data', download=True, transform=train_transform, target_transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(np.arange(len(train_dataset)), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    FishDataset('../data', transform=test_transform, target_transform=test_transform),\n",
    "    batch_size=32,\n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard(outputs, targets):\n",
    "    outputs = outputs.view(outputs.size(0), -1)\n",
    "    targets = targets.view(targets.size(0), -1)\n",
    "    intersection = (outputs * targets).sum(1)\n",
    "    union = (outputs + targets).sum(1) - intersection\n",
    "    jac = (intersection + 0.001) / (union + 0.001)\n",
    "    return jac.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (down1): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (down2): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (down3): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (middle): conv_block(\n",
       "    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (leaky_relu): LeakyReLU(0.01)\n",
       "  )\n",
       "  (up3): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (up2): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       "  (up1): Sequential(\n",
       "    (0): conv_block(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "    (1): conv_block(\n",
       "      (conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (leaky_relu): LeakyReLU(0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_folder = os.path.abspath('../models')\n",
    "if not os.path.exists(model_folder):\n",
    "    os.mkdir(model_folder)\n",
    "model_path = os.path.join(model_folder, 'unet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/5\n",
      "    batch   1/685 loss: 0.7597, jaccard 0.1310\n",
      "    batch  51/685 loss: 0.6546, jaccard 0.6962\n",
      "    batch 101/685 loss: 0.6477, jaccard 0.7860\n",
      "    batch 151/685 loss: 0.6474, jaccard 0.7801\n",
      "    batch 201/685 loss: 0.6405, jaccard 0.8275\n",
      "    batch 251/685 loss: 0.6364, jaccard 0.8467\n",
      "    batch 301/685 loss: 0.6396, jaccard 0.8081\n",
      "    batch 351/685 loss: 0.6308, jaccard 0.8697\n",
      "    batch 401/685 loss: 0.6329, jaccard 0.8403\n",
      "    batch 451/685 loss: 0.6293, jaccard 0.8602\n",
      "    batch 501/685 loss: 0.6279, jaccard 0.8722\n",
      "    batch 551/685 loss: 0.6297, jaccard 0.8639\n",
      "    batch 601/685 loss: 0.6294, jaccard 0.8621\n",
      "    batch 651/685 loss: 0.6205, jaccard 0.8920\n",
      "Finished epoch 1, starting evaluation\n",
      "    loss: 0.6381  jaccard: 0.8154            val_loss: 0.6274 val_jaccard: 0.8766\n",
      "\n",
      "Starting epoch 2/5\n",
      "    batch   1/685 loss: 0.6228, jaccard 0.8836\n",
      "    batch  51/685 loss: 0.6248, jaccard 0.8651\n",
      "    batch 101/685 loss: 0.6259, jaccard 0.8638\n",
      "    batch 151/685 loss: 0.6236, jaccard 0.8764\n",
      "    batch 201/685 loss: 0.6231, jaccard 0.8814\n",
      "    batch 251/685 loss: 0.6231, jaccard 0.8788\n",
      "    batch 301/685 loss: 0.6223, jaccard 0.8793\n",
      "    batch 351/685 loss: 0.6178, jaccard 0.9005\n",
      "    batch 401/685 loss: 0.6190, jaccard 0.8915\n",
      "    batch 451/685 loss: 0.6218, jaccard 0.8824\n",
      "    batch 501/685 loss: 0.6208, jaccard 0.8881\n",
      "    batch 551/685 loss: 0.6185, jaccard 0.8938\n",
      "    batch 601/685 loss: 0.6250, jaccard 0.8801\n",
      "    batch 651/685 loss: 0.6249, jaccard 0.8721\n",
      "Finished epoch 2, starting evaluation\n",
      "    loss: 0.6233  jaccard: 0.8786            val_loss: 0.6207 val_jaccard: 0.8883\n",
      "\n",
      "Starting epoch 3/5\n",
      "    batch   1/685 loss: 0.6170, jaccard 0.8874\n",
      "    batch  51/685 loss: 0.6205, jaccard 0.8924\n",
      "    batch 101/685 loss: 0.6252, jaccard 0.8714\n",
      "    batch 151/685 loss: 0.6192, jaccard 0.8835\n",
      "    batch 201/685 loss: 0.6179, jaccard 0.8774\n",
      "    batch 251/685 loss: 0.6166, jaccard 0.9028\n",
      "    batch 301/685 loss: 0.6184, jaccard 0.8931\n",
      "    batch 351/685 loss: 0.6176, jaccard 0.8855\n",
      "    batch 401/685 loss: 0.6184, jaccard 0.8910\n",
      "    batch 451/685 loss: 0.6180, jaccard 0.8913\n",
      "    batch 501/685 loss: 0.6147, jaccard 0.8963\n",
      "    batch 551/685 loss: 0.6183, jaccard 0.9003\n",
      "    batch 601/685 loss: 0.6175, jaccard 0.8936\n",
      "    batch 651/685 loss: 0.6220, jaccard 0.8853\n",
      "Finished epoch 3, starting evaluation\n",
      "    loss: 0.6194  jaccard: 0.8882            val_loss: 0.6186 val_jaccard: 0.8856\n",
      "\n",
      "Starting epoch 4/5\n",
      "    batch   1/685 loss: 0.6154, jaccard 0.9014\n",
      "    batch  51/685 loss: 0.6153, jaccard 0.9080\n",
      "    batch 101/685 loss: 0.6146, jaccard 0.9078\n",
      "    batch 151/685 loss: 0.6188, jaccard 0.8717\n",
      "    batch 201/685 loss: 0.6168, jaccard 0.8856\n",
      "    batch 251/685 loss: 0.6151, jaccard 0.8938\n",
      "    batch 301/685 loss: 0.6171, jaccard 0.8906\n",
      "    batch 351/685 loss: 0.6155, jaccard 0.8950\n",
      "    batch 401/685 loss: 0.6106, jaccard 0.9106\n",
      "    batch 451/685 loss: 0.6208, jaccard 0.8844\n",
      "    batch 501/685 loss: 0.6158, jaccard 0.8982\n",
      "    batch 551/685 loss: 0.6194, jaccard 0.8981\n",
      "    batch 601/685 loss: 0.6161, jaccard 0.8912\n",
      "    batch 651/685 loss: 0.6158, jaccard 0.8964\n",
      "Finished epoch 4, starting evaluation\n",
      "    loss: 0.6164  jaccard: 0.8924            val_loss: 0.6146 val_jaccard: 0.8952\n",
      "\n",
      "Starting epoch 5/5\n",
      "    batch   1/685 loss: 0.6130, jaccard 0.8998\n",
      "    batch  51/685 loss: 0.6131, jaccard 0.8983\n",
      "    batch 101/685 loss: 0.6143, jaccard 0.8942\n",
      "    batch 151/685 loss: 0.6146, jaccard 0.9045\n",
      "    batch 201/685 loss: 0.6188, jaccard 0.8719\n",
      "    batch 251/685 loss: 0.6146, jaccard 0.8931\n",
      "    batch 301/685 loss: 0.6120, jaccard 0.8870\n",
      "    batch 351/685 loss: 0.6087, jaccard 0.9090\n",
      "    batch 401/685 loss: 0.6129, jaccard 0.8918\n",
      "    batch 451/685 loss: 0.6132, jaccard 0.8809\n",
      "    batch 501/685 loss: 0.6136, jaccard 0.8864\n",
      "    batch 551/685 loss: 0.6093, jaccard 0.8916\n",
      "    batch 601/685 loss: 0.6102, jaccard 0.9043\n",
      "    batch 651/685 loss: 0.6140, jaccard 0.8954\n",
      "Finished epoch 5, starting evaluation\n",
      "    loss: 0.6133  jaccard: 0.8948            val_loss: 0.6111 val_jaccard: 0.8984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = {'loss': [], 'jaccard': [], 'val_loss': [], 'val_jaccard': []}\n",
    "num_epochs = 5\n",
    "display_steps = 50\n",
    "best_jaccard = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "    # train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_jaccard = 0.0\n",
    "    for batch_idx, (images, masks, _) in enumerate(train_loader):\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.round()\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        jac = jaccard(outputs.round(), masks)\n",
    "        running_jaccard += jac.data[0]\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if batch_idx % display_steps == 0:\n",
    "            print('    ', end='')\n",
    "            print('batch {:>3}/{:>3} loss: {:.4f}, jaccard {:.4f}\\r'\\\n",
    "                  .format(batch_idx+1, len(train_loader),\n",
    "                          loss.data[0], jac.data[0]))\n",
    "\n",
    "        \n",
    "    # evalute\n",
    "    print('Finished epoch {}, starting evaluation'.format(epoch+1))\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_jaccard = 0.0\n",
    "    for images, masks, _ in val_loader:\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        val_running_loss += loss.data[0]\n",
    "        jac = jaccard(outputs.round(), masks)\n",
    "        val_running_jaccard += jac.data[0]\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_jaccard = running_jaccard / len(train_loader)\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_jaccard = val_running_jaccard / len(val_loader)\n",
    "    \n",
    "    hist['loss'].append(train_loss)\n",
    "    hist['jaccard'].append(train_jaccard)\n",
    "    hist['val_loss'].append(val_loss)\n",
    "    hist['val_jaccard'].append(val_jaccard)\n",
    "    \n",
    "    if val_jaccard > best_jaccard:\n",
    "        torch.save(model, model_path)\n",
    "    print('    ', end='')\n",
    "    print('loss: {:.4f}  jaccard: {:.4f} \\\n",
    "           val_loss: {:.4f} val_jaccard: {:4.4f}\\n'\\\n",
    "           .format(train_loss, train_jaccard, val_loss, val_jaccard))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
